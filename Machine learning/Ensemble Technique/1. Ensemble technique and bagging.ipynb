{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532e370c",
   "metadata": {},
   "source": [
    "## Ensemble Technique\n",
    "\n",
    "`Ensemble techniques` are machine learning methods that combine multiple individual models to create a more powerful and accurate predictive model. The idea behind ensemble techniques is that by aggregating the predictions of multiple models, the strengths of individual models can be leveraged to compensate for their weaknesses, resulting in a more robust and accurate prediction.\n",
    "\n",
    "There are several popular ensemble techniques, including:\n",
    "\n",
    "1. `Bagging`: Bagging stands for Bootstrap Aggregating. It involves training multiple instances of the same base model on different subsets of the training data, obtained through bootstrap sampling (sampling with replacement). Each model is trained independently, and their predictions are combined through averaging or voting to make the final prediction.\n",
    "\n",
    "2. `Boosting`: Boosting is an iterative ensemble technique that aims to improve the performance of a weak base model by training multiple models in sequence. Each subsequent model focuses on correcting the mistakes made by the previous models, with more emphasis on the data points that were misclassified. The final prediction is typically a weighted combination of the predictions made by each individual model.\n",
    "\n",
    "3. `Random Forest`: Random Forest is an ensemble method that combines the ideas of bagging and decision trees. It creates an ensemble of decision trees, where each tree is trained on a random subset of the training data and a random subset of the features. The predictions of the individual trees are then aggregated through majority voting to make the final prediction.\n",
    "\n",
    "4. `Stacking`: Stacking, also known as stacked generalization, involves training multiple diverse models and combining their predictions using another model called a meta-learner or blender. The base models are trained on the training data, and their predictions serve as inputs to the meta-learner, which learns to make the final prediction based on the outputs of the base models.\n",
    "\n",
    "Ensemble techniques are effective because they exploit the diversity and complementary strengths of individual models. By combining multiple models, ensemble methods can reduce overfitting, increase generalization performance, and improve the robustness of predictions. They have been successfully applied in various machine learning tasks, including classification, regression, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccebc6",
   "metadata": {},
   "source": [
    "### Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the performance and stability of predictive models. It involves creating multiple instances of the same base model by training them on different subsets of the training data obtained through bootstrap sampling.\n",
    "\n",
    "### Here's a step-by-step explanation of the Bagging process:\n",
    "\n",
    "- 1. `Bootstrap Sampling:`  Given a training dataset with N samples, Bagging generates B bootstrap samples by randomly selecting N samples from the original dataset with replacement. This means that each bootstrap sample can contain duplicate instances and some original instances may be left out.\n",
    "\n",
    "- 2. `Base Model Training:` For each bootstrap sample, a base model (e.g., decision tree, random forest, or neural network) is trained independently. Each base model is trained on a different subset of the training data, capturing different variations and patterns in the data.\n",
    "\n",
    "- 3. `Prediction Aggregation`: Once the base models are trained, they are used to make predictions on new unseen data. The predictions from all the base models are then aggregated to make the final prediction. The aggregation can be done through averaging (for regression tasks) or voting (for classification tasks).\n",
    "\n",
    "The key idea behind Bagging is that by training multiple models on different subsets of the data, the ensemble can reduce overfitting and variance in the predictions. The individual models are exposed to different patterns and noise in the data, leading to diverse predictions. When these predictions are combined, the errors tend to cancel out, resulting in a more accurate and robust prediction.\n",
    "\n",
    "One popular type of Bagging is `Random Forest`, which combines the concepts of Bagging and decision trees. In Random Forest, each base model is a decision tree trained on a random subset of the training data and a random subset of the features. The predictions of the individual trees are then aggregated through majority voting to make the final prediction.\n",
    "\n",
    "`Bagging and Random Forest` are widely used in machine learning because they are relatively simple yet effective techniques for improving model performance, handling noisy data, and reducing overfitting. They have been applied to various tasks such as classification, regression, and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4434cd8",
   "metadata": {},
   "source": [
    "## Random Forest Classifier And Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c7afc",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Random Forest lies in the combination of the bootstrap sampling technique and the ensemble of decision trees. Let's break down the key components:\n",
    "\n",
    "- `Bootstrap Sampling:` Random Forest starts by creating multiple bootstrap samples from the original training data. Each bootstrap sample is created by randomly selecting data points from the original dataset with replacement. This means that some instances may appear multiple times in a bootstrap sample, while others may be left out.\n",
    "\n",
    "The mathematical intuition behind bootstrap sampling is rooted in statistics and probability. By creating multiple bootstrap samples, we simulate different possible training datasets that can be generated from the original data. This allows us to capture different variations and distributions of the data, which helps in reducing overfitting and increasing the robustness of the model.\n",
    "\n",
    "- `Decision Trees:` Random Forest uses decision trees as the base model. Decision trees are binary tree structures that recursively split the data based on features to create a hierarchical decision-making process.\n",
    "\n",
    "Each decision tree in Random Forest is trained on a different bootstrap sample. When training a decision tree, at each split, a subset of features is randomly selected. This random feature selection helps to introduce diversity among the trees and prevents them from being highly correlated.\n",
    "\n",
    "The mathematical intuition behind decision trees lies in the algorithm's ability to partition the feature space based on the training data. Decision trees learn to make decisions by evaluating features and creating binary splits that minimize impurity or maximize information gain. This process can be represented mathematically using various impurity measures or objective functions, such as Gini index or entropy.\n",
    "\n",
    "- `Ensemble Aggregation:` The final prediction in Random Forest is obtained by aggregating the predictions of all the individual decision trees. For classification tasks, this aggregation is typically done through majority voting, where the class with the most votes is chosen as the final prediction. For regression tasks, the predictions are averaged.\n",
    "\n",
    "The mathematical intuition behind ensemble aggregation is based on the principle of combining multiple weak learners to create a stronger predictor. Each decision tree in the Random Forest may have its biases and limitations, but by combining their predictions, the ensemble can capture a more accurate representation of the underlying patterns and relationships in the data.\n",
    "\n",
    "In summary, the mathematical intuition behind Random Forest comes from the principles of bootstrap sampling, the decision-making process of decision trees, and the aggregation of their predictions. Through these components, Random Forest leverages the diversity and collective knowledge of multiple decision trees to create a more robust and accurate model for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333c7e2",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Random Forest's use of both raw sampling (bootstrap sampling) and feature sampling lies in the principles of ensemble learning and reducing overfitting.\n",
    "\n",
    "Raw Sampling (Bootstrap Sampling): Random Forest uses raw sampling, specifically bootstrap sampling, to create multiple subsets of the training data. Bootstrap sampling involves randomly selecting data points from the original training set with replacement. As a result, each bootstrap sample may contain duplicate instances and some original instances may be left out. This sampling technique ensures that each base model (decision tree) in the Random Forest ensemble is trained on a slightly different subset of the training data.\n",
    "The mathematical intuition behind raw sampling lies in the concept of the law of large numbers. When we generate multiple bootstrap samples, each sample is likely to represent the original training data with some variations. By training each base model on these different subsets, Random Forest captures diverse patterns and noise present in the data. The ensemble benefits from the collective knowledge of the individual models, resulting in a more robust and accurate prediction.\n",
    "\n",
    "Feature Sampling: In addition to raw sampling, Random Forest also employs feature sampling. Feature sampling involves randomly selecting a subset of features (predictors) at each split when constructing a decision tree. Rather than considering all the available features at each split, only a subset of features is considered.\n",
    "The mathematical intuition behind feature sampling is to introduce randomness and reduce the correlation between base models (decision trees). When all features are available for selection at each split, strong features tend to dominate the decision-making process, resulting in highly correlated trees and potentially overfitting. By randomly selecting a subset of features, Random Forest encourages diversity among the base models. Each decision tree focuses on different sets of features, resulting in a set of less correlated trees. This reduces the risk of overfitting and enhances the ensemble's generalization capability.\n",
    "\n",
    "The combination of raw sampling and feature sampling in Random Forest aims to strike a balance between individual model diversity and predictive accuracy. By leveraging the diverse perspectives captured through raw sampling and feature sampling, Random Forest can effectively handle complex and noisy datasets, reduce overfitting, and provide robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9af4a",
   "metadata": {},
   "source": [
    "## Out of Bags score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22601762",
   "metadata": {},
   "source": [
    "Ensemble methods in machine learning, such as bagging, are designed to improve the predictive accuracy and stability of models by combining the predictions of multiple individual models. The intuition behind bagging lies in the concept of the \"wisdom of the crowd,\" where aggregating the opinions of a group of individuals often leads to better results than relying on a single individual.\n",
    "\n",
    "Bagging, short for bootstrap aggregating, involves creating multiple subsets of the original training data through a process called bootstrapping. Bootstrapping is a statistical technique where random samples are drawn with replacement from the original dataset, resulting in subsets of the same size as the original dataset. Each subset is used to train a separate base model, typically referred to as weak learners or base learners.\n",
    "\n",
    "The mathematical intuition behind bagging can be understood in the context of reducing both bias and variance in model predictions. Bias refers to the error introduced by approximating a real-world problem with a simplified model. Variance, on the other hand, refers to the variability of model predictions when trained on different subsets of the training data.\n",
    "\n",
    "When multiple base models are trained using different bootstrapped subsets, they collectively capture different aspects of the underlying data distribution. Each model may have its biases, but by combining their predictions, these biases tend to cancel each other out, resulting in an overall reduction in bias.\n",
    "\n",
    "In terms of variance, since each base model is trained on a different subset of the data, they are exposed to different variations and noise present in the training data. By averaging or combining their predictions, the impact of individual outliers or noisy samples is reduced. This leads to a decrease in the overall variability or variance of the ensemble's predictions compared to a single model.\n",
    "\n",
    "Mathematically, the bagging process can be described as follows:\n",
    "\n",
    "- 1. Let's assume we have a training dataset of size N.\n",
    "- 2.  Generate B bootstrap samples by randomly selecting N samples from the original dataset with replacement. Each bootstrap sample has the same size as the original dataset.\n",
    "- 3. Train a separate base model on each bootstrap sample, resulting in B different base models.\n",
    "- 4. When making predictions, each base model independently predicts the output for a given input.\n",
    "- 5. For regression problems, the final prediction is often computed as the average of the predictions from all base models. For classification problems, voting or averaging probabilities is commonly used.\n",
    "\n",
    "The ensemble's prediction is the aggregation of the individual predictions from the base models.\n",
    "\n",
    "The key idea behind the mathematical intuition of bagging is that by combining the predictions of multiple models trained on different subsets of the data, the ensemble model achieves a better overall generalization and robustness, which can lead to improved performance in terms of accuracy and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d7bb3",
   "metadata": {},
   "source": [
    "## RandomForest Classifier Implementation With pipeline And HyperParameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d259a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset(\"tips\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778277fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dinner', 'Lunch']\n",
       "Categories (2, object): ['Lunch', 'Dinner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"time\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b200ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_bill    0\n",
       "tip           0\n",
       "sex           0\n",
       "smoker        0\n",
       "day           0\n",
       "time          0\n",
       "size          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac3ef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_bill float64\n",
      "tip float64\n",
      "sex category\n",
      "smoker category\n",
      "day category\n",
      "time category\n",
      "size int64\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i, df[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98adf5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df[\"time\"] = encoder.fit_transform(df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ccebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    176\n",
       "1     68\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a71b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels=[\"time\"], axis=1)\n",
    "y = df[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6867dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd0603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>13.28</td>\n",
       "      <td>2.72</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>24.27</td>\n",
       "      <td>2.03</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>27.28</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>31.71</td>\n",
       "      <td>4.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>15.98</td>\n",
       "      <td>2.03</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip   sex smoker   day  size\n",
       "228       13.28  2.72  Male     No   Sat     2\n",
       "208       24.27  2.03  Male    Yes   Sat     2\n",
       "96        27.28  4.00  Male    Yes   Fri     2\n",
       "167       31.71  4.50  Male     No   Sun     4\n",
       "84        15.98  2.03  Male     No  Thur     2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9705009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17215ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"sex\", \"smoker\", \"day\"]\n",
    "numerical_cols = [\"total_bill\", \"tip\", \"size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6405d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6a17c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('imputer',SimpleImputer(strategy=\"most_frequent\")),\n",
    "        ('onehotencoding', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcd7a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"num_pipeline\", num_pipeline, numerical_cols),\n",
    "    ('cat_pipeline', cat_pipeline, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321bff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8383838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f53875",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\" : RandomForestClassifier(),\n",
    "    \"DecisionTree\"  : DecisionTreeClassifier(), \n",
    "    \"SVC\" : SVC()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "892b1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abc47dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, X_test, y_train, y_test, models):\n",
    "    \n",
    "    report = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = list(models.values())[i]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        test_model_score = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        report[list(models.keys())[i]] = test_model_score\n",
    "        \n",
    "    return report    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12a8078a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random Forest': 0.9591836734693877,\n",
       " 'DecisionTree': 0.9387755102040817,\n",
       " 'SVC': 0.9591836734693877}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(X_train, X_test, y_train, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1ae2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4370326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Training\n",
    "params = {\n",
    "    \"max_depth\":[3,5,10,None],\n",
    "    \"n_estimators\":[100,200,300],\n",
    "    \"criterion\":['gini','entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "477f83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58685689",
   "metadata": {},
   "outputs": [],
   "source": [
    "RndomizedSearchCV(classification, param_distributions=params,scoring=\"accuracy\",cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "063736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm= RandomizedSearchCV(classification, param_distributions=params,scoring=\"accuracy\",cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e32df7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, n_estimators=100;, score=0.974 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, n_estimators=100;, score=0.923 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, n_estimators=100;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, n_estimators=100;, score=0.897 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, n_estimators=100;, score=0.949 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.974 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.923 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=100;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.949 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=100;, score=0.923 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=200;, score=0.974 total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=200;, score=0.923 total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=200;, score=1.000 total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=200;, score=0.949 total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=200;, score=0.923 total time=   0.6s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.974 total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.923 total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=1.000 total time=   2.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.949 total time=   1.6s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=300;, score=0.923 total time=   1.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=300;, score=0.974 total time=   1.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=300;, score=0.923 total time=   1.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=300;, score=1.000 total time=   0.9s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=300;, score=0.949 total time=   1.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=300;, score=0.923 total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, n_estimators=100;, score=0.974 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, n_estimators=100;, score=0.949 total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, n_estimators=100;, score=0.974 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, n_estimators=100;, score=0.923 total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, n_estimators=100;, score=0.923 total time=   0.3s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, n_estimators=100;, score=0.974 total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, n_estimators=100;, score=0.923 total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, n_estimators=100;, score=0.974 total time=   0.3s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, n_estimators=100;, score=0.949 total time=   0.3s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, n_estimators=100;, score=0.923 total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.974 total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.923 total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=200;, score=1.000 total time=   0.7s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.897 total time=   0.7s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=200;, score=0.923 total time=   0.6s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.974 total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.923 total time=   0.6s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=1.000 total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.949 total time=   0.6s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, n_estimators=200;, score=0.923 total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, n_estimators=300;, score=0.974 total time=   1.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, n_estimators=300;, score=0.923 total time=   1.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, n_estimators=300;, score=0.974 total time=   1.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, n_estimators=300;, score=0.923 total time=   0.9s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, n_estimators=300;, score=0.923 total time=   1.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 10, None],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 10, None],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [3, 5, 10, None],\n",
       "                                        'n_estimators': [100, 200, 300]},\n",
       "                   scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00ecf525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100, 'max_depth': None, 'criterion': 'gini'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c68753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
